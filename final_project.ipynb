{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "573e6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ralphmatzner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8a023999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The load_data function loads the nlp dta from the relevant resource. \n",
    "Returns: \n",
    "    list: A list of strings for further processing is returned.\n",
    "    list: A list of strings for further processing is returned.\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    # https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "    data = []\n",
    "    with open('./IMDB_Dataset.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "        for review in reader:\n",
    "            data.append(review[0:-1])\n",
    "        \n",
    "        # To allow for model checking and hyper parameter tuning the dataset \n",
    "        # has been abbridged as there is not enough time to train it properly on 25000 samples.\n",
    "        # Comment out to train on full dataset.\n",
    "        data = data[0:10]\n",
    "        data_test = data[10:20]\n",
    "        \n",
    "        # Uncomment for full dataset.\n",
    "        #data = [0:25000]\n",
    "        #data_test = data[25000:]\n",
    "        \n",
    "    return data, data_test\n",
    "    \n",
    "\"\"\"\n",
    "The preprocess_data method applies the following preprocessing steps to the data, tokenization.\n",
    "Params:\n",
    "    data (list): The data is given as a list of strings to be further processed.\n",
    "Returns:\n",
    "    list: A list of preprocessed strings is returned.\n",
    "\"\"\"\n",
    "def preprocess_data(data):\n",
    "    pp_words = []\n",
    "    for text in data:\n",
    "        tokens = word_tokenize(text[0])\n",
    "        pp_words.append(tokens)\n",
    "    return pp_words\n",
    "    \n",
    "\"\"\"\n",
    "The embed_data method embeds the words into a n^k plain.\n",
    "Params:\n",
    "    data (list): A list of strings that have been tokenized.\n",
    "    context_size (int): The contexts size of the window.  \n",
    "Returns:\n",
    "    dict: The word embedings.\n",
    "    list of sets: The ngrams.\n",
    "\"\"\"\n",
    "def embed_data(data, context_size, embedded_dict):\n",
    "    word_embed = embedded_dict\n",
    "    ngrams = []\n",
    "    for text in data:\n",
    "        vocab = set(text)\n",
    "        words = {word: i for i, word in enumerate(vocab)}\n",
    "        z = {**words, **word_embed}\n",
    "        word_embed = z\n",
    "    \n",
    "        for i in range(len(text) - context_size):\n",
    "            tup = [text[j] for j in np.arange(i , i + context_size) ]\n",
    "            ngrams.append((tup, text[i + context_size]))\n",
    "        \n",
    "    return word_embed, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d46e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Word2Vec class defines a word2vec model with dimensions and structure as follows.\n",
    "The structure is an embedding layer, followed by a linear layer with ReLU activation function and\n",
    "an output layer which is a linear layer with softmax activation function. The dimensions are \n",
    "n_words in and embedding dim out, followed by context_size*embed_dim in and 128 out and 128 in \n",
    "and n_words out.\n",
    "\"\"\"\n",
    "class Word2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    The init function generates the initial state of the model and takes the following parameters.\n",
    "    Parameters:\n",
    "        n_words (int): The number of words in the vocab dictionary.\n",
    "        embed_dim (int): The embedding dimension as an int.\n",
    "        context_size (int): The context window size as an int.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_words, embed_dim, context_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.n_words = n_words\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_words, embed_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embed_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, n_words)\n",
    "       \n",
    "    \"\"\"\n",
    "    The forward function performs a forward pass on the model with the relevant piece of text.\n",
    "    Parameters:\n",
    "        sentence (set of ngrams and word pairings): A set of ngrams and word pairings or a single \n",
    "        ngram and word pairing.\n",
    "    Returns:\n",
    "        list: Returns the model output for the given pieve or text (set of ngrams).\n",
    "    \"\"\"\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence).view((1, -1))  \n",
    "        out1 = F.relu(self.linear1(embeds))\n",
    "        out2 = self.linear2(out1)\n",
    "        softmax = F.log_softmax(out2, dim=1)\n",
    "        \n",
    "        return softmax\n",
    "        \n",
    "    \"\"\"\n",
    "    The predict function takes an ngram and predicts the output of the model for that ngram.\n",
    "    Predict only works for single inputs.\n",
    "    Parameters:\n",
    "        sentence (set of ngrams and word pairings): A set of ngrams and word pairings or a single \n",
    "        ngram and word pairing.\n",
    "        word_window (dict): A dictionary containing all words mapped to a unique int.\n",
    "    Returns:\n",
    "        list: Returns the model output for the given pieve or text (set of ngrams).\n",
    "    \"\"\"\n",
    "    def predict(self, sentence, word_window):\n",
    "        #print(sentence[0])\n",
    "        context_vecs = torch.tensor([word_window[word] for word in sentence], dtype=torch.long)\n",
    "        out = model(context_vecs)\n",
    "        return out\n",
    "    \n",
    "    \"\"\"\n",
    "    The graph function takes losses and epochs and generates a graph of the loss as it decreases over time.\n",
    "    Parameters:\n",
    "        losses (list): The given losses.\n",
    "        epochs (int): The number of epochs.\n",
    "    \"\"\"\n",
    "    def graph(self, losses, epochs):\n",
    "        y_val = []\n",
    "        [y_val.append(epoch) for epoch in range(0,epochs,1)]\n",
    "        plt.plot(y_val, losses)\n",
    "        plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    The train function takes ngrams and an embed_dict and trains the model parameters for n epochs.\n",
    "    Params:\n",
    "        ngrams (list of sets): A list of sets of ngram and the associated word.\n",
    "        embed_dict (dict): A dict of words in the vocab.\n",
    "        epochs (int): An int describing the number of epochs to be used.\n",
    "    \"\"\"\n",
    "    def train(self, ngrams, embed_dict, epochs):\n",
    "        print('Started training.')\n",
    "        # Train model.\n",
    "        losses = []\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(0,epochs,1):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for context, target in ngrams:# necessary to expand outside train to pass in as data.\n",
    "                \n",
    "                # Prepare inputs for model.\n",
    "                context_vecs = torch.tensor([embed_dict[word] for word in context], dtype=torch.long)\n",
    "                \n",
    "                # Zero gradients.\n",
    "                model.zero_grad()\n",
    "                \n",
    "                # Run forward pass to get log probabilities.\n",
    "                softmax = model(context_vecs)\n",
    "                \n",
    "                # Compute loss.\n",
    "                loss = loss_function(softmax, torch.tensor([embed_dict[target]], dtype=torch.long))\n",
    "                \n",
    "                # Run the backward pass and update the gradients.\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            print('Epoch ' + str(epoch) + ' complete.')\n",
    "            losses.append(total_loss)\n",
    "        \n",
    "        self.graph(losses, epochs)\n",
    "        print('Ended training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1abd0980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been preprocessed.\n",
      "Stage 1 complete.\n",
      "Stage 2 complete.\n",
      "12407.501374721527\n",
      "Stage 3 complete.\n",
      "Started training.\n",
      "Epoch 0 complete.\n",
      "Epoch 1 complete.\n",
      "Epoch 2 complete.\n",
      "Epoch 3 complete.\n",
      "Epoch 4 complete.\n",
      "Epoch 5 complete.\n",
      "Epoch 6 complete.\n",
      "Epoch 7 complete.\n",
      "Epoch 8 complete.\n",
      "Epoch 9 complete.\n",
      "Epoch 10 complete.\n",
      "Epoch 11 complete.\n",
      "Epoch 12 complete.\n",
      "Epoch 13 complete.\n",
      "Epoch 14 complete.\n",
      "Epoch 15 complete.\n",
      "Epoch 16 complete.\n",
      "Epoch 17 complete.\n",
      "Epoch 18 complete.\n",
      "Epoch 19 complete.\n",
      "Epoch 20 complete.\n",
      "Epoch 21 complete.\n",
      "Epoch 22 complete.\n",
      "Epoch 23 complete.\n",
      "Epoch 24 complete.\n",
      "Epoch 25 complete.\n",
      "Epoch 26 complete.\n",
      "Epoch 27 complete.\n",
      "Epoch 28 complete.\n",
      "Epoch 29 complete.\n",
      "Epoch 30 complete.\n",
      "Epoch 31 complete.\n",
      "Epoch 32 complete.\n",
      "Epoch 33 complete.\n",
      "Epoch 34 complete.\n",
      "Epoch 35 complete.\n",
      "Epoch 36 complete.\n",
      "Epoch 37 complete.\n",
      "Epoch 38 complete.\n",
      "Epoch 39 complete.\n",
      "Epoch 40 complete.\n",
      "Epoch 41 complete.\n",
      "Epoch 42 complete.\n",
      "Epoch 43 complete.\n",
      "Epoch 44 complete.\n",
      "Epoch 45 complete.\n",
      "Epoch 46 complete.\n",
      "Epoch 47 complete.\n",
      "Epoch 48 complete.\n",
      "Epoch 49 complete.\n",
      "Epoch 50 complete.\n",
      "Epoch 51 complete.\n",
      "Epoch 52 complete.\n",
      "Epoch 53 complete.\n",
      "Epoch 54 complete.\n",
      "Epoch 55 complete.\n",
      "Epoch 56 complete.\n",
      "Epoch 57 complete.\n",
      "Epoch 58 complete.\n",
      "Epoch 59 complete.\n",
      "Epoch 60 complete.\n",
      "Epoch 61 complete.\n",
      "Epoch 62 complete.\n",
      "Epoch 63 complete.\n",
      "Epoch 64 complete.\n",
      "Epoch 65 complete.\n",
      "Epoch 66 complete.\n",
      "Epoch 67 complete.\n",
      "Epoch 68 complete.\n",
      "Epoch 69 complete.\n",
      "Epoch 70 complete.\n",
      "Epoch 71 complete.\n",
      "Epoch 72 complete.\n",
      "Epoch 73 complete.\n",
      "Epoch 74 complete.\n",
      "Epoch 75 complete.\n",
      "Epoch 76 complete.\n",
      "Epoch 77 complete.\n",
      "Epoch 78 complete.\n",
      "Epoch 79 complete.\n",
      "Epoch 80 complete.\n",
      "Epoch 81 complete.\n",
      "Epoch 82 complete.\n",
      "Epoch 83 complete.\n",
      "Epoch 84 complete.\n",
      "Epoch 85 complete.\n",
      "Epoch 86 complete.\n",
      "Epoch 87 complete.\n",
      "Epoch 88 complete.\n",
      "Epoch 89 complete.\n",
      "Epoch 90 complete.\n",
      "Epoch 91 complete.\n",
      "Epoch 92 complete.\n",
      "Epoch 93 complete.\n",
      "Epoch 94 complete.\n",
      "Epoch 95 complete.\n",
      "Epoch 96 complete.\n",
      "Epoch 97 complete.\n",
      "Epoch 98 complete.\n",
      "Epoch 99 complete.\n",
      "Epoch 100 complete.\n",
      "Epoch 101 complete.\n",
      "Epoch 102 complete.\n",
      "Epoch 103 complete.\n",
      "Epoch 104 complete.\n",
      "Epoch 105 complete.\n",
      "Epoch 106 complete.\n",
      "Epoch 107 complete.\n",
      "Epoch 108 complete.\n",
      "Epoch 109 complete.\n",
      "Epoch 110 complete.\n",
      "Epoch 111 complete.\n",
      "Epoch 112 complete.\n",
      "Epoch 113 complete.\n",
      "Epoch 114 complete.\n",
      "Epoch 115 complete.\n",
      "Epoch 116 complete.\n",
      "Epoch 117 complete.\n",
      "Epoch 118 complete.\n",
      "Epoch 119 complete.\n",
      "Epoch 120 complete.\n",
      "Epoch 121 complete.\n",
      "Epoch 122 complete.\n",
      "Epoch 123 complete.\n",
      "Epoch 124 complete.\n",
      "Epoch 125 complete.\n",
      "Epoch 126 complete.\n",
      "Epoch 127 complete.\n",
      "Epoch 128 complete.\n",
      "Epoch 129 complete.\n",
      "Epoch 130 complete.\n",
      "Epoch 131 complete.\n",
      "Epoch 132 complete.\n",
      "Epoch 133 complete.\n",
      "Epoch 134 complete.\n",
      "Epoch 135 complete.\n",
      "Epoch 136 complete.\n",
      "Epoch 137 complete.\n",
      "Epoch 138 complete.\n",
      "Epoch 139 complete.\n",
      "Epoch 140 complete.\n",
      "Epoch 141 complete.\n",
      "Epoch 142 complete.\n",
      "Epoch 143 complete.\n",
      "Epoch 144 complete.\n",
      "Epoch 145 complete.\n",
      "Epoch 146 complete.\n",
      "Epoch 147 complete.\n",
      "Epoch 148 complete.\n",
      "Epoch 149 complete.\n",
      "Epoch 150 complete.\n",
      "Epoch 151 complete.\n",
      "Epoch 152 complete.\n",
      "Epoch 153 complete.\n",
      "Epoch 154 complete.\n",
      "Epoch 155 complete.\n",
      "Epoch 156 complete.\n",
      "Epoch 157 complete.\n",
      "Epoch 158 complete.\n",
      "Epoch 159 complete.\n",
      "Epoch 160 complete.\n",
      "Epoch 161 complete.\n",
      "Epoch 162 complete.\n",
      "Epoch 163 complete.\n",
      "Epoch 164 complete.\n",
      "Epoch 165 complete.\n",
      "Epoch 166 complete.\n",
      "Epoch 167 complete.\n",
      "Epoch 168 complete.\n",
      "Epoch 169 complete.\n",
      "Epoch 170 complete.\n",
      "Epoch 171 complete.\n",
      "Epoch 172 complete.\n",
      "Epoch 173 complete.\n",
      "Epoch 174 complete.\n",
      "Epoch 175 complete.\n",
      "Epoch 176 complete.\n",
      "Epoch 177 complete.\n",
      "Epoch 178 complete.\n",
      "Epoch 179 complete.\n",
      "Epoch 180 complete.\n",
      "Epoch 181 complete.\n",
      "Epoch 182 complete.\n",
      "Epoch 183 complete.\n",
      "Epoch 184 complete.\n",
      "Epoch 185 complete.\n",
      "Epoch 186 complete.\n",
      "Epoch 187 complete.\n",
      "Epoch 188 complete.\n",
      "Epoch 189 complete.\n",
      "Epoch 190 complete.\n",
      "Epoch 191 complete.\n",
      "Epoch 192 complete.\n",
      "Epoch 193 complete.\n",
      "Epoch 194 complete.\n",
      "Epoch 195 complete.\n",
      "Epoch 196 complete.\n",
      "Epoch 197 complete.\n",
      "Epoch 198 complete.\n",
      "Epoch 199 complete.\n",
      "Epoch 200 complete.\n",
      "Epoch 201 complete.\n",
      "Epoch 202 complete.\n",
      "Epoch 203 complete.\n",
      "Epoch 204 complete.\n",
      "Epoch 205 complete.\n",
      "Epoch 206 complete.\n",
      "Epoch 207 complete.\n",
      "Epoch 208 complete.\n",
      "Epoch 209 complete.\n",
      "Epoch 210 complete.\n",
      "Epoch 211 complete.\n",
      "Epoch 212 complete.\n",
      "Epoch 213 complete.\n",
      "Epoch 214 complete.\n",
      "Epoch 215 complete.\n",
      "Epoch 216 complete.\n",
      "Epoch 217 complete.\n",
      "Epoch 218 complete.\n",
      "Epoch 219 complete.\n",
      "Epoch 220 complete.\n",
      "Epoch 221 complete.\n",
      "Epoch 222 complete.\n",
      "Epoch 223 complete.\n",
      "Epoch 224 complete.\n",
      "Epoch 225 complete.\n",
      "Epoch 226 complete.\n",
      "Epoch 227 complete.\n",
      "Epoch 228 complete.\n",
      "Epoch 229 complete.\n",
      "Epoch 230 complete.\n",
      "Epoch 231 complete.\n",
      "Epoch 232 complete.\n",
      "Epoch 233 complete.\n",
      "Epoch 234 complete.\n",
      "Epoch 235 complete.\n",
      "Epoch 236 complete.\n",
      "Epoch 237 complete.\n",
      "Epoch 238 complete.\n",
      "Epoch 239 complete.\n",
      "Epoch 240 complete.\n",
      "Epoch 241 complete.\n",
      "Epoch 242 complete.\n",
      "Epoch 243 complete.\n",
      "Epoch 244 complete.\n",
      "Epoch 245 complete.\n",
      "Epoch 246 complete.\n",
      "Epoch 247 complete.\n",
      "Epoch 248 complete.\n",
      "Epoch 249 complete.\n",
      "Epoch 250 complete.\n",
      "Epoch 251 complete.\n",
      "Epoch 252 complete.\n",
      "Epoch 253 complete.\n",
      "Epoch 254 complete.\n",
      "Epoch 255 complete.\n",
      "Epoch 256 complete.\n",
      "Epoch 257 complete.\n",
      "Epoch 258 complete.\n",
      "Epoch 259 complete.\n",
      "Epoch 260 complete.\n",
      "Epoch 261 complete.\n",
      "Epoch 262 complete.\n",
      "Epoch 263 complete.\n",
      "Epoch 264 complete.\n",
      "Epoch 265 complete.\n",
      "Epoch 266 complete.\n",
      "Epoch 267 complete.\n",
      "Epoch 268 complete.\n",
      "Epoch 269 complete.\n",
      "Epoch 270 complete.\n",
      "Epoch 271 complete.\n",
      "Epoch 272 complete.\n",
      "Epoch 273 complete.\n",
      "Epoch 274 complete.\n",
      "Epoch 275 complete.\n",
      "Epoch 276 complete.\n",
      "Epoch 277 complete.\n",
      "Epoch 278 complete.\n",
      "Epoch 279 complete.\n",
      "Epoch 280 complete.\n",
      "Epoch 281 complete.\n",
      "Epoch 282 complete.\n",
      "Epoch 283 complete.\n",
      "Epoch 284 complete.\n",
      "Epoch 285 complete.\n",
      "Epoch 286 complete.\n",
      "Epoch 287 complete.\n",
      "Epoch 288 complete.\n",
      "Epoch 289 complete.\n",
      "Epoch 290 complete.\n",
      "Epoch 291 complete.\n",
      "Epoch 292 complete.\n",
      "Epoch 293 complete.\n",
      "Epoch 294 complete.\n",
      "Epoch 295 complete.\n",
      "Epoch 296 complete.\n",
      "Epoch 297 complete.\n",
      "Epoch 298 complete.\n",
      "Epoch 299 complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended training.\n",
      "Stage 4 complete.\n"
     ]
    }
   ],
   "source": [
    "# Load data and preprocess. In theory Cross validation or\n",
    "# Bagging should be applied to determin quality of the model \n",
    "# and to help with hyperparameter selection. But due to the \n",
    "# computational intensity of the dataset and the model and the \n",
    "# limited amount of time, these have not been performed.\n",
    "data, data_test = load_data()\n",
    "pp_data = preprocess_data(data)\n",
    "pp_data_test = preprocess_data(data_test)\n",
    "print('Data has been preprocessed.')\n",
    "\n",
    "context_size = 3\n",
    "embed_dict, ngrams = embed_data(pp_data, context_size, {})\n",
    "embed_dict, ngrams_test = embed_data(pp_data, context_size, embed_dict)\n",
    "print('Stage 1 complete.')\n",
    "\n",
    "# Build model and prepare input params.\n",
    "n_words = len(embed_dict) #check if this is correct.\n",
    "embed_dim = 10\n",
    "\n",
    "model = Word2Vec(n_words, embed_dim, context_size)\n",
    "\n",
    "print('Stage 2 complete.')\n",
    "\n",
    "# Train model.\n",
    "#model.train(ngrams, embed_dict, 100)\n",
    "l = 0\n",
    "for ng,test in ngrams_test:\n",
    "    out = model.predict(ng, embed_dict)\n",
    "    loss = nn.NLLLoss()\n",
    "    l+= loss(out, torch.tensor([embed_dict[test]], dtype=torch.long)).item()\n",
    "print(l)\n",
    "print('Stage 3 complete.')\n",
    "\n",
    "# Optimize Hyperparameters. The only hyperparameter to think of is \n",
    "# lambda for regularisation or the number of epochs. Since we can \n",
    "# only optimise the number of epochs hyperparameter selection has not been performed.\n",
    "ls = []\n",
    "\n",
    "def optims(ls, model, epochs):\n",
    "    model.train(ngrams, embed_dict, epochs)\n",
    "    l = 0\n",
    "    for ng,test in ngrams_test:\n",
    "        out = model.predict(ng, embed_dict)\n",
    "        loss = nn.NLLLoss()\n",
    "        l+= loss(out, torch.tensor([embed_dict[test]], dtype=torch.long)).item()\n",
    "        ls.append(l)\n",
    "\n",
    "# Hyperparameter tuning: \n",
    "# Commented out for computation reasons.\n",
    "#model = Word2Vec(n_words, embed_dim, context_size)\n",
    "#optims(ls, model, 10)\n",
    "\n",
    "#model = Word2Vec(n_words, embed_dim, context_size)\n",
    "#optims(ls, model, 100)\n",
    "\n",
    "#model = Word2Vec(n_words, embed_dim, context_size)\n",
    "#optims(ls, model, 200)\n",
    "\n",
    "#model = Word2Vec(n_words, embed_dim, context_size)\n",
    "#optims(ls, model, 300)\n",
    "\n",
    "model = Word2Vec(n_words, embed_dim, context_size)\n",
    "model.train(ngrams, embed_dict, 300)\n",
    "torch.save(model.state_dict(), './model')\n",
    "print('Stage 4 complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba49cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26848958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
